#+latex_header: \input{header.tex}
#+TITLE: An introduction to applied probabilistic inference
#+STARTUP: latexpreview

* An introduction to applied probabilistic inference
:PROPERTIES:
:CUSTOM_ID: an-introduction-to-applied-probabilistic-inference
:END:

These notes borrow heavily from and are based on

- Betancourt, Michael (2019). Probabilistic Modeling and Statistical
  Inference. Retrieved from
  [[https://github.com/betanalpha/knitr_case_studies/tree/master/modeling_and_inference]],
  commit b474ec1a5a79347f7c9634376c866fe3294d657a.
- Betancourt, Michael (2020). Towards A Principled Bayesian Workflow
  (RStan). Retrieved from
  [[https://github.com/betanalpha/knitr_case_studies/tree/master/principled_bayesian_workflow]],
  commit 23eb263be4cfb44278d0dfb8ddbd593a4b142506.
- [[https://github.com/betanalpha/knitr_case_studies][betanalpha/knitr_case_studies]]
- [[https://github.com/lstmemery/principled-bayesian-workflow-pymc3][lstmemery/principled-bayesian-workflow-pymc3]]

* System setup
** local
*** setup remote container host machine

We assume here we have already setup a container named =notebooks-vm= so all we need to do to begin with is to start it up.

#+BEGIN_SRC sh :results output verbatim replace :exports both
gcloud compute instances start notebooks-vm
#+END_SRC

#+RESULTS:

Check that our instance is indeed running

#+BEGIN_SRC sh :results output verbatim replace :exports both
gcloud compute instances list
#+END_SRC

#+RESULTS:
: NAME          ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP   STATUS
: notebooks     us-central1-c  n1-standard-1               10.128.0.22                TERMINATED
: notebooks-vm  us-central1-f  n1-standard-1  true         10.128.0.26  34.72.75.223  RUNNING

Make sure the correct ip address is entered into our =.ssh/config= file.
#+BEGIN_SRC sh :results output verbatim replace :exports both
gcloud compute config-ssh
#+END_SRC

#+RESULTS:
: You should now be able to use ssh/scp with your instances.
: For example, try running:
:
:   $ ssh notebooks-vm.us-central1-f.quarere
:

Inspect the IP address we find in our =.ssh/config= file

#+BEGIN_SRC sh :results output verbatim replace :exports both
grep HostName ~/.ssh/config
#+END_SRC

#+RESULTS:
:     HostName 35.224.59.240
:     HostName 34.72.75.223

*** transfer files to remote machine

#+BEGIN_SRC sh :results output verbatim replace :exports both
scp plotting.py
#+END_SRC

** remote
:PROPERTIES:
:header-args: :results output verbatim replace :session notebookscontainer-sh-pbw :dir /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:  :exports both  :eval never-export
:END:

Establish a connection to the container remote host.

#+BEGIN_SRC sh :results silent
echo $HOSTNAME
#+END_SRC

#+RESULTS:
: notebooks-vm

#+BEGIN_SRC sh :results silent
sudo pacman -Syu --needed --noconfirm otf-latin-modern wget
wget https://raw.githubusercontent.com/lstmemery/principled-bayesian-workflow-pymc3/master/data.csv
#+END_SRC

* Plotting setup
:PROPERTIES:
:header-args: :results output verbatim replace :session notebookscontainer-pbw :dir /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:  :exports both  :eval never-export
:CUSTOM_ID: plotting-setup
:END:

The following is a template that can be used in case it is important to time the execution of a given block.

#+BEGIN_SRC python
import time, datetime
start_time = time.time()
time.sleep(1)
print("--- %s seconds ---" % (time.time() - start_time))
print(datetime.datetime.now())
#+END_SRC

#+RESULTS:
: --- 1.0011107921600342 seconds ---
: 2020-10-30 02:26:30.861428

This is used to set the default plotting fonts to =Latin Modern=.

#+BEGIN_SRC python
start_time = time.time()
import matplotlib.pyplot as plt
import matplotlib.font_manager
matplotlib.font_manager.fontManager.addfont("/usr/share/fonts/OTF/lmsans10-regular.otf")
matplotlib.font_manager.fontManager.addfont("/usr/share/fonts/OTF/lmroman10-regular.otf")
plt.style.use('default') #reset default parameters
# https://stackoverflow.com/a/3900167/446907
plt.rcParams.update({'font.size': 16,
                     'font.family': ['sans-serif'],
                     'font.serif': ['Latin Modern Roman'] + plt.rcParams['font.serif'],
                     'font.sans-serif': ['Latin Modern Sans'] + plt.rcParams['font.sans-serif']})
print("--- %s seconds ---" % (time.time() - start_time))
print(datetime.datetime.now())
#+END_SRC

#+RESULTS:
: --- 4.768032073974609 seconds ---
: 2020-10-30 02:27:31.530687

* Modeling process
  :PROPERTIES:
  :CUSTOM_ID: modeling-process
  :END:
** The world through the lens of probability
   :PROPERTIES:
   :CUSTOM_ID: the-world-through-the-lens-of-probability
   :END:
*** Systems, environments, and observations
:PROPERTIES:
:CUSTOM_ID: systems-environments-and-observations
:END:

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :float t :width 0.8\textwidth
#+CAPTION: Schematic of different probes attempting to measure properties of a given phenomenon within its embedding environment.
[[file:img/multiple_probes.png]]

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :float t :width 0.8\textwidth
#+CAPTION: Schematic of different observational processes designed to capture features of a given phenomenon.
[[file:img/multiple_observational_processes.png]]

*** The space of observational models and the true data generating process
:PROPERTIES:
:CUSTOM_ID: the-space-of-observational-models-and-the-true-data-generating-process
:END:
**** The observational model
:PROPERTIES:
:CUSTOM_ID: the-observational-model
:END:

- observation space: $Y$
- arbitrary points in the observation space: $y$
- explicitly realized observations from the observational process
$\tilde{y}$
- data generating process: a probability distribution over the observation space
- space of all data generating processes: $\mathcal{P}$
- observational model vs model configuration space: the subspace,        $\mathcal{S} \subset \mathcal{P}$, of data generating processes considered in any particular application
- parametrization: a map from a model configuration space $\mathcal{S}$ to a parameter space $\mathcal{\Theta}$ assigning to each model configuration $s \in \mathcal{S}$ a parameter $\theta \in \mathcal{\Theta}$
- probability density for an observational model: $\pi_{\mathcal{S}}(y; s)$ in general using the parametrization to assign $\pi_{\mathcal{S}}(y; \theta)$

#+ATTR_ORG: :width 450
#+ATTR_LATEX: :float t :width 0.5\textwidth
#+CAPTION: The space of data generating processes with the subspace of model configurations along with the true data generating process in the event that it is contained within the subspace of model configurations.
[[file:img/small_world.png]]

#+ATTR_ORG: :width 450
#+ATTR_LATEX: :float t :width 0.5\textwidth
#+CAPTION: The space of data generating processes with the subspace of model configurations along with the true data generating process in the event that it is NOT contained within the subspace of model configurations.
[[file:img/small_world_one.png]]


**** The true data generating process
     :PROPERTIES:
     :CUSTOM_ID: the-true-data-generating-process
     :END:

     - true data generating process: $\pi^{\dagger}$ is the probability
       distribution that exactly captures the observational process in a
       given application

** The practical reality of model construction
   :PROPERTIES:
   :CUSTOM_ID: the-practical-reality-of-model-construction
   :END:

#+ATTR_ORG: :width 900
[[file:img/small_world_two.png]]

** The process of inference
:PROPERTIES:
:CUSTOM_ID: the-process-of-inference
:END:

#+ATTR_ORG: :width 900
[[file:img/model_config5.png]]

possible to providing a high quality mirror for natural systems?

* Workflow overview
:PROPERTIES:
:CUSTOM_ID: workflow-overview
:END:

#+ATTR_ORG: :width 900
[[file:img/all.png]]

* Load libraries
:PROPERTIES:
:header-args: :results output verbatim replace :session notebookscontainer-pbw :dir /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:  :exports both  :eval never-export
:CUSTOM_ID: load-libraries
:END:

#+BEGIN_SRC python
# %pylab inline # <-- this may be useful in jupyter
# import matplotlib.pyplot as plt
import pymc3 as pm
import pandas as pd
import scipy.stats as stats
import seaborn as sns
import theano.tensor as T
import theano
import numpy as np
# plt.style.use(['seaborn-talk'])
# plt.rcParams["figure.figsize"] = (10,8)
print(pm.__version__)
print(theano.__version__)
#+END_SRC

#+RESULTS:
: 3.9.3
: 1.0.5

** define colors
:PROPERTIES:
:CUSTOM_ID: define-colors
:END:

#+BEGIN_SRC python
c_light ="#DCBCBC"
c_light_highlight ="#C79999"
c_mid ="#B97C7C"
c_mid_highlight ="#A25050"
c_dark ="#8F2727"
c_dark_highlight ="#7C0000"
#+END_SRC

#+RESULTS:

* Section 4.1
:PROPERTIES:
:header-args: :results output verbatim replace :session notebookscontainer-pbw :dir /ssh:notebooks-vm.us-central1-f.quarere|docker:klt-notebooks-vm-cjme:  :exports both  :eval never-export
:CUSTOM_ID: section-41
:END:

Build a model that generates (Poisson) counts that may explain what we have in our data

** Example generative models
:PROPERTIES:
:CUSTOM_ID: example-generative-models
:END:
*** Univariate normal model
:PROPERTIES:
:CUSTOM_ID: univariate-normal-model
:END:

From a very simple perspective, generative modeling refers to the situation in which we develop a candidate probabilistic specification of the process from which our data are generated. Usually this will include the specification of prior distributions over all first-order parameters.

#+ATTR_ORG: :width 200
#+ATTR_LATEX: :float t :width 0.25\textwidth
#+CAPTION: Factor graph for a univariate normal model
[[file:img/tikz-57bc0c88a2974f4c1e2335fe9edb88ff2efdf970.png]]

\begin{equation}
\begin{split}
p(\mathbf{y}|\mu,\tau) &= \prod^{9}_{n=0} \mathcal{N}(y_n|\mu,\tau) p(\mu) \\
                        &= \mathcal{N}(\mu|0,10^{-6}) \\
p(\tau)                &= \mathcal{G}(\tau|10^{-6},10^{-6})
\end{split}
\end{equation}

This happens to come from a library called [[https://github.com/bayespy/bayespy/blob/develop/doc/source/user_guide/quickstart.rst][bayespy]]. The best description I am aware of the syntax and semantics of graphical models via factor graph notation is in the [[https://github.com/jluttine/tikz-bayesnet][tikz-bayesnet]] library [[https://github.com/jluttine/tikz-bayesnet/blob/master/dietz-techreport.pdf][technical report]].

*** Multivariate normal models
:PROPERTIES:
:CUSTOM_ID: multivariate-normal-models
:END:

#+ATTR_ORG: :width 200
[[file:img/tikz-80a1db369be1f25b61ceacfff551dae2bdd331c3.png]]

$$\mathbf{y}_{mn} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Lambda}),\qquad m=0,\ldots,9, \quad n=0,\ldots,29.$$

#+ATTR_ORG: :width 300
[[file:img/tikz-97236981a2be663d10ade1ad85caa727621615db.png]]

$$\mathbf{y}_{mn} \sim \mathcal{N}(\boldsymbol{\mu}_m, \mathbf{\Lambda}_n),\qquad m=0,\ldots,9, \quad n=0,\ldots,29.$$

Note that these are for illustrative purposes of the manner in which our data can share parameters and we have not yet defined priors over our parameters.

** Our data
:PROPERTIES:
:CUSTOM_ID: our-data
:END:

#+BEGIN_SRC python
df = pd.read_csv('data.csv')
print(df.head(7))
print(df.shape)
#+END_SRC

#+RESULTS:
:   data  value
: 0    y      5
: 1    y     10
: 2    y      7
: 3    y      0
: 4    y      0
: 5    y      0
: 6    y      9
: (1001, 2)

** Build a generative model
:PROPERTIES:
:CUSTOM_ID: build-a-generative-model
:END:

#+BEGIN_SRC python
lbda  = np.linspace(0, 20, num=int(20/0.001))

plt.plot(lbda, stats.norm(loc=0,scale=6.44787).pdf(lbda), c=c_dark_highlight, lw=2)
plt.xlabel("lambda"); plt.ylabel("Prior Density"); plt.yticks([]);

lbda99 = np.linspace(0, 15, num=int(15/0.001))
plt.fill_between(lbda99,0.,y2=stats.norm(loc=0,scale=6.44787).pdf(lbda99),color=c_dark);
plt.savefig("fig/prior-density-lambda.svg", bbox_inches="tight");
# !inkscape fig/prior-density-lambda.svg --export-filename=fig/prior-density-lambda.pdf;
#+END_SRC

In this case the complete Bayesian model is given by

$$
\pi( y_{1}, \ldots, y_{N}, \lambda )
=
\left[ \prod_{n = 1}^{N} \text{Poisson} (y_{n} \mid \lambda) \right]
\cdot \text{HalfNormal} (\lambda \mid 6).
$$

#+ATTR_ORG: :width 900
[[file:img/dgm.png]]

#+BEGIN_SRC python
#WORKING
model = pm.Model()
N = 1000
R = 500
with model:
    lbda = pm.HalfNormal("lbda",sd=6.44787)
    y = pm.Poisson("y",mu=lbda,shape=(N,),observed=None)
#+END_SRC

#+BEGIN_SRC python
with model:
    trace = pm.sample_prior_predictive(samples=R)
#+END_SRC

#+BEGIN_SRC python
simu_lbdas = trace['lbda']
simu_ys = trace['y']
#+END_SRC

#+BEGIN_SRC python
print(simu_lbdas[0:9])
print(simu_lbdas.shape)
#+END_SRC

#+BEGIN_SRC python
print(simu_ys[0:9])
print(simu_ys.shape)
#+END_SRC

** Plot prior predictive distribution
  :PROPERTIES:
  :CUSTOM_ID: plot-prior-predictive-distribution
  :END:

#+BEGIN_SRC python
x_max = 30
bins = np.arange(0,x_max)
bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 1, simu_ys)

prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=0)
prctiles_interp = np.repeat(prctiles, 10,axis=1)
#+END_SRC

#+BEGIN_SRC python
for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
    plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);

plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
plt.axvline(x=25,ls='-',lw=2,color='k');
plt.xlabel('y');
plt.title('Prior predictive distribution');
#+END_SRC

** Fit to simulated data
:PROPERTIES:
:CUSTOM_ID: fit-to-simulated-data
:END:

Betancourt performs this for each =y= in trace. For now we just do it for a single one.

#+BEGIN_SRC python
model = pm.Model()
with model:
   lbda = pm.HalfNormal("lbda",sd=6.44787)

   y = pm.Poisson("y",mu=lbda,shape=(N,),observed=simu_ys[-1,:])

   trace = pm.sample(draws=R,tune=4*R)

#+END_SRC


#+BEGIN_SRC python
pm.plots.traceplot(trace);
#+END_SRC

#+ATTR_ORG: :width 900
[[file:img/prior_post_regimes.png]]

#+ATTR_ORG: :width 900
[[file:img/eye_chart_regimes.png]]

Posterior z-score
$$z[f \mid \tilde{y}, \theta^{\dagger}] =
\frac{ \mathbb{E}_{\mathrm{post}}[f \mid \tilde{y}] - f(\theta^{\dagger}) }
{ \mathbb{E}_{\mathrm{post}}[f \mid \tilde{y} ] },$$

Posterior contraction
$$
c[f \mid \tilde{y}] = 1 -
\frac{ \mathbb{V}_{\mathrm{post}}[f \mid \tilde{y} ] }
{ \mathbb{V}_{\mathrm{prior}}[f \mid \tilde{y} ] },
$$

#+BEGIN_SRC python
# Compute rank of prior draw with respect to thinned posterior draws
sbc_rank = np.sum(simu_lbdas < trace['lbda'][::2])
#+END_SRC

#+BEGIN_SRC python
# posterior sensitivities analysis
s = pm.stats.summary(trace,varnames=['lbda'])
post_mean_lbda = s['mean'].values
post_sd_lbda = s['sd'].values
prior_sd_lbda = 6.44787
z_score = np.abs((post_mean_lbda - simu_lbdas) / post_sd_lbda)
shrinkage = 1 - (post_sd_lbda / prior_sd_lbda ) ** 2
#+END_SRC

#+BEGIN_SRC python
plt.plot(shrinkage[0]*np.ones(len(z_score)),z_score,'o',c="#8F272720");
plt.xlim(0,1.01); plt.xlabel('Posterior shrinkage'); plt.ylabel('Posterior z-score');
#+END_SRC

** Fit observations and evaluate
   :PROPERTIES:
   :CUSTOM_ID: fit-observations-and-evaluate
   :END:

   #+BEGIN_SRC python
   df = pd.read_csv('data.csv')
   data_ys = df[df['data']=='y']['value'].values
   #+END_SRC

   #+BEGIN_SRC python
   model = pm.Model()
     with model:
      lbda = pm.HalfNormal("lbda",sd=6.44787)

      y = pm.Poisson("y",mu=lbda,shape=(N,),observed=data_ys)

      trace = pm.sample(draws=R,tune=4*R,chains=4)
   #+END_SRC


  #+BEGIN_SRC python
  pm.plots.plot_posterior(trace,varnames=['lbda']);
  #+END_SRC

  #+BEGIN_SRC python
  with model:
       ppc = pm.sample_posterior_predictive(trace)
  #+END_SRC

  #+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 1, ppc['y'])

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=0)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)

  data_hist = np.histogram(data_ys,bins=bins)[0]
  data_hist_interp = np.repeat(data_hist, 10)
  #+END_SRC

  #+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.plot(bin_interp,data_hist_interp,color='black');
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Posterior predictive distribution');
  #+END_SRC

* Section 4.2
:PROPERTIES:
:CUSTOM_ID: section-42
:END:

#+BEGIN_SRC python
generative_ensemble2 = pm.Model()
N = 1000
R = 1000

with generative_ensemble2:
    theta = pm.Beta(name="theta", alpha = 1, beta = 1)
    lambda_ = pm.HalfNormal(name="lambda", sd = 6.44787)
    y = pm.ZeroInflatedPoisson(name = "y", psi = theta, theta = lambda_, shape = (N,))
#+END_SRC

  #+BEGIN_SRC python
  with generative_ensemble2:
      trace = pm.sample_prior_predictive(samples=R)
  #+END_SRC

  #+BEGIN_SRC python
  trace["theta"][:10]
  #+END_SRC

  #+BEGIN_SRC python
  trace["lambda"][:10]
  #+END_SRC

  #+BEGIN_SRC python
  simu_ys = trace["y"]
  simu_ys
  #+END_SRC

  #+BEGIN_SRC python
  np.count_nonzero(simu_ys, axis=0).std()
  #+END_SRC


  #+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0 ,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)

  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 1, simu_ys.T)

  prctiles = np.percentile(hists,np.linspace(10, 90,num=9),axis=0)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)



  for i, color in enumerate([c_light, c_light_highlight, c_mid, c_mid_highlight]):
      plt.fill_between(bin_interp, prctiles_interp[i, :],
                       prctiles_interp[-1 - i, :],
                       alpha = 1.0,
                       color = color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Prior predictive distribution');
  #+END_SRC


  #+BEGIN_SRC python
  simu_ys[simu_ys > 25].size / simu_ys.size
  #+END_SRC

** Fit Simulated Observations and Evaluate
   :PROPERTIES:
   :CUSTOM_ID: fit-simulated-observations-and-evaluate
   :END:

  #+BEGIN_SRC python
  fit_data2 = pm.Model()

  N = 1000
  R = 1000

  with fit_data2:
      theta = pm.Beta(name="theta", alpha = 1, beta = 1)
      lambda_ = pm.HalfNormal(name="lambda", sd = 6.44787)
      y = pm.ZeroInflatedPoisson(name = "y", 
                                 psi = theta, 
                                 theta = lambda_, 
                                 shape = (N,),
                                 observed=simu_ys[-1,:])
  #+END_SRC

  #+BEGIN_SRC python
  with fit_data2:
      trace_fit = pm.sample(R)
  #+END_SRC


  #+BEGIN_SRC python
  pm.plots.traceplot(trace_fit)
  #+END_SRC


  #+BEGIN_SRC python
  pm.summary(trace_fit, varnames=["theta", "lambda"]).round(2)
  #+END_SRC



  #+BEGIN_SRC python
  import pickle
  with open("fit_data2.pkl", "wb+") as buffer:
      pickle.dump({"model": fit_data2, "trace": trace_fit}, buffer)
  #+END_SRC

* Section 4.3
:PROPERTIES:
:CUSTOM_ID: section-43
:END:

Build a model that generates zero-inflated Poisson counts

** Build a generative model
:PROPERTIES:
:CUSTOM_ID: build-a-generative-model
:END:

#+BEGIN_SRC python
lbda  = np.linspace(0, 20, num=int(20/0.001))
pdf = stats.invgamma(3.48681,scale=9.21604)
plt.plot(lbda, pdf.pdf(lbda), c=c_dark_highlight, lw=2)
plt.xlabel("lambda"); plt.ylabel("Prior Density"); plt.yticks([]);

lbda99 = np.linspace(1, 15, num=int(15/0.001))

plt.fill_between(lbda99,0.,y2=pdf.pdf(lbda99),color=c_dark)
#+END_SRC


#+BEGIN_SRC python
theta  = np.linspace(0, 1, num=int(1/0.001))
pdf = stats.beta(2.8663,2.8663)
plt.plot(theta, pdf.pdf(theta), c=c_dark_highlight, lw=2)
plt.xlabel("theta"); plt.ylabel("Prior Density"); plt.yticks([]);

theta99 = np.linspace(0.1, 0.9, num=int(0.8/0.001))

plt.fill_between(theta99,0.,y2=pdf.pdf(theta99),color=c_dark)
#+END_SRC


#+BEGIN_SRC python
  #WORKING

model = pm.Model()
N = 1000
R = 1000
with model:
    lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
    theta = pm.Beta("theta",alpha=2.8663,beta=2.8663)
      
    y = pm.ZeroInflatedPoisson("y",psi=theta,theta=lbda,shape=N)
      
#+END_SRC

#+BEGIN_SRC python
  # Note this breaks when N != R
with model:
    trace = pm.sample_prior_predictive(samples=R)
#+END_SRC

#+BEGIN_SRC python
simu_lbdas = trace['lbda']
simu_thetas = trace['theta']
simu_ys = trace['y']
#+END_SRC

** Plot prior predictive distribution
   :PROPERTIES:
   :CUSTOM_ID: plot-prior-predictive-distribution
   :END:

  #+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 0, simu_ys)

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=1)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)
  #+END_SRC

  #+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Prior predictive distribution');
  #+END_SRC

** Fit to simulated data
   :PROPERTIES:
   :CUSTOM_ID: fit-to-simulated-data
   :END:

   In example Betancourt performs this for each =y= in trace. Here we just do it for one.

  #+BEGIN_SRC python
  model = pm.Model()
  with model:
      lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
      theta = pm.Beta("theta",alpha=2.8663,beta=2.8663)
      
      y = pm.ZeroInflatedPoisson("y",psi=theta,theta=lbda,shape=N,observed=simu_ys[:,-1])
      
      trace = pm.sample(draws=R,tune=4*R)
     
  #+END_SRC


  #+BEGIN_SRC python
  pm.plots.traceplot(trace);
  #+END_SRC


  #+BEGIN_SRC python
  # Compute rank of prior draw with respect to thinned posterior draws
  sbc_rank = np.sum(simu_lbdas < trace['lbda'][::2])
  #+END_SRC

  #+BEGIN_SRC python
  # posterior sensitivities analysis
  s = pm.stats.summary(trace,varnames=['lbda'])
  post_mean_lbda = s['mean'].values
  post_sd_lbda = s['sd'].values
  prior_sd_lbda = 6.44787
  z_score = np.abs((post_mean_lbda - simu_lbdas) / post_sd_lbda)
  shrinkage = 1 - (post_sd_lbda / prior_sd_lbda ) ** 2
  #+END_SRC

  #+BEGIN_SRC python
  plt.plot(shrinkage[0]*np.ones(len(z_score)),z_score,'o',c="#8F272720");
  plt.xlim(0,1.01); plt.xlabel('Posterior shrinkage'); plt.ylabel('Posterior z-score');
  #+END_SRC

** Fit observations and evaluate
   :PROPERTIES:
   :CUSTOM_ID: fit-observations-and-evaluate
   :END:

  #+BEGIN_SRC python
  df = pd.read_csv('data.csv')
  data_ys = df[df['data']=='y']['value'].values
  #+END_SRC

  #+BEGIN_SRC python
  model = pm.Model()
  with model:
      lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
      theta = pm.Beta("theta",alpha=2.8663,beta=2.8663)
      
      y = pm.ZeroInflatedPoisson("y",psi=theta,theta=lbda,shape=N,observed=data_ys)
      
      trace = pm.sample(draws=R,tune=4*R,chains=4)
  #+END_SRC


  #+BEGIN_SRC python
  pm.plots.plot_posterior(trace,varnames=['lbda']);
  #+END_SRC


  #+BEGIN_SRC python
  with model:
       ppc = pm.sample_ppc(trace)
  #+END_SRC


  #+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 0, ppc['y'])

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=1)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)

  data_hist = np.histogram(data_ys,bins=bins)[0]
  data_hist_interp = np.repeat(data_hist, 10)
  #+END_SRC

  #+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.plot(bin_interp,data_hist_interp,color='black');
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Posterior predictive distribution');
  #+END_SRC

* Section 4.4
  :PROPERTIES:
  :CUSTOM_ID: section-44
  :END:

  #+BEGIN_SRC python
  from pymc3.distributions.distribution import generate_samples,draw_values,Discrete
  from pymc3.distributions.discrete import Poisson

  def rv_truncated_poisson(mu,mx, size=None):
      mu = np.asarray(mu)
      mx = np.asarray(mx)
      dist = stats.distributions.poisson(mu)

      lower_cdf = 0.
      upper_cdf = dist.cdf(mx)
      nrm = upper_cdf - lower_cdf
      sample = np.random.random_sample(size) * nrm + lower_cdf

      return dist.ppf(sample)

  class TruncatedZeroInflatedPoisson(Discrete):

      def __init__(self, mu, mx, psi, *args, **kwargs):
          super(TruncatedZeroInflatedPoisson, self).__init__(*args, **kwargs)
          self.mu  = tt.as_tensor_variable(mu)
          self.mx = tt.as_tensor_variable(mx)
          self.psi = tt.as_tensor_variable(psi)
          self.mode = tt.floor(mu).astype('int32')


      def random(self, point=None, size=None):
          mu, psi, mx = draw_values([self.mu, self.psi, self.mx], point=point, size=size)
          g = generate_samples(rv_truncated_poisson, mu,mx,
                               dist_shape=self.shape,
                               size=size)
          return g * (np.random.random(np.squeeze(g.shape)) < psi)

      def logp(self, value):
          psi = self.psi
          mu = self.mu
          mx = self.mx
          poisson = pm.Poisson.dist(mu)
          logp_val = tt.switch(
              tt.gt(value, 0),
              tt.log(psi) + poisson.logp(value),
              pm.math.logaddexp(tt.log1p(-psi), tt.log(psi) - mu))

          return pm.distributions.dist_math.bound(
              logp_val,
              0 <= value,
              value <= mx,
              0 <= psi, psi <= 1,
              0 <= mu)
  #+END_SRC

  #+BEGIN_SRC python
  model = pm.Model()
  N = 1000
  R = 1000
  with model:
      lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
      psi = pm.Beta("psi",alpha=2.8663,beta=2.8663)
      
      y = TruncatedZeroInflatedPoisson("y",psi=psi,mu=lbda,mx=15.,shape=N)
  #+END_SRC

  #+BEGIN_SRC python
  with model:
      trace = pm.sample_prior_predictive(samples=1000)
  #+END_SRC

  #+BEGIN_SRC python
  simu_lbdas = trace['lbda']
  simu_thetas = trace['psi']
  simu_ys = trace['y']
  #+END_SRC

  #+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 0, simu_ys)

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=1)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)
  #+END_SRC

  #+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Prior predictive distribution');
  #+END_SRC


  #+BEGIN_SRC python
  model = pm.Model()
  N = 1000
  R = 1000
  with model:
      lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
      psi = pm.Beta("psi",alpha=2.8663,beta=2.8663)
      
      y = TruncatedZeroInflatedPoisson("y",psi=psi,mu=lbda,mx=14.,shape=N,observed=data_ys)
      trace = pm.sample(draws=R,tune=4*R,chains=4)    
  #+END_SRC


  #+BEGIN_SRC python
  pm.plots.plot_posterior(trace);
  #+END_SRC


  #+BEGIN_SRC python
  with model:
       ppc = pm.sample_ppc(trace)
  #+END_SRC


  #+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 0, ppc['y'])

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=1)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)

  data_hist = np.histogram(data_ys,bins=bins)[0]
  data_hist_interp = np.repeat(data_hist, 10)
  #+END_SRC

  #+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.plot(bin_interp,data_hist_interp,color='black');
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Posterior predictive distribution');
  #+END_SRC
