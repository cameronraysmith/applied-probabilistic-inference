#+latex_header: \input{header.tex}
#+TITLE: An introduction to applied probabilistic inference
#+STARTUP: latexpreview

* An introduction to applied probabilistic inference
  :PROPERTIES:
  :CUSTOM_ID: an-introduction-to-applied-probabilistic-inference
  :END:

These notes borrow heavily from and are based on

- Betancourt, Michael (2019). Probabilistic Modeling and Statistical
  Inference. Retrieved from
  [[https://github.com/betanalpha/knitr_case_studies/tree/master/modeling_and_inference]],
  commit b474ec1a5a79347f7c9634376c866fe3294d657a.
- Betancourt, Michael (2020). Towards A Principled Bayesian Workflow
  (RStan). Retrieved from
  [[https://github.com/betanalpha/knitr_case_studies/tree/master/principled_bayesian_workflow]],
  commit 23eb263be4cfb44278d0dfb8ddbd593a4b142506.
- [[https://github.com/betanalpha/knitr_case_studies][betanalpha/knitr_case_studies]]
- [[https://github.com/lstmemery/principled-bayesian-workflow-pymc3][lstmemery/principled-bayesian-workflow-pymc3]]

* Plotting setup
  :PROPERTIES:
  :CUSTOM_ID: plotting-setup
  :END:

#+BEGIN_SRC python
  %run -i 'plotting.py'
#+END_SRC

* Modeling process
  :PROPERTIES:
  :CUSTOM_ID: modeling-process
  :END:

** The world through the lens of probability
   :PROPERTIES:
   :CUSTOM_ID: the-world-through-the-lens-of-probability
   :END:

*** Systems, environments, and observations
    :PROPERTIES:
    :CUSTOM_ID: systems-environments-and-observations
    :END:

#+ATTR_ORG: :width 900
[[file:img/multiple_probes.png]]

#+ATTR_ORG: :width 900
[[file:img/multiple_observational_processes.png]]


*** The space of observational models and the true data generating
process
    :PROPERTIES:
    :CUSTOM_ID: the-space-of-observational-models-and-the-true-data-generating-process
    :END:

**** The observational model
     :PROPERTIES:
     :CUSTOM_ID: the-observational-model
     :END:

- observation space: $Y$
- arbitrary points in the observation space: $y$
- explicitly realized observations from the observational process
  $\tilde{y}$
- data generating process: a probability distribution over the
  observation space
- space of all data generating processes: $\mathcal{P}$
- observational model vs model configuration space: the subspace,
  $\mathcal{S} \subset \mathcal{P}$, of data generating processes
  considered in any particular application
- parametrization: a map from a model configuration space $\mathcal{S}$
  to a parameter space $\mathcal{\Theta}$ assigning to each model
  configuration $s \in \mathcal{S}$ a parameter
  $\theta \in \mathcal{\Theta}$
- probability density for an observational model:
  $\pi_{\mathcal{S}}(y; s)$ in general using the parametrization to
  assign $\pi_{\mathcal{S}}(y; \theta)$


#+ATTR_ORG: :width 450
[[file:img/small_world.png]]

#+ATTR_ORG: :width 450
[[file:img/small_world_one.png]]


#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://github.com/betanalpha/knitr_case_studies/raw/master/modeling_and_inference/figures/small_world/small_world/small_world.png" alt="Drawing" width="45%"/>
  <img src="https://github.com/betanalpha/knitr_case_studies/raw/master/modeling_and_inference/figures/small_world/small_world_one/small_world_one.png" alt="Drawing" width="45%"/>
  </center>
  </div>
#+END_HTML

**** The true data generating process
     :PROPERTIES:
     :CUSTOM_ID: the-true-data-generating-process
     :END:

- true data generating process: $\pi^{\dagger}$ is the probability
  distribution that exactly captures the observational process in a
  given application

** The practical reality of model construction
   :PROPERTIES:
   :CUSTOM_ID: the-practical-reality-of-model-construction
   :END:

#+ATTR_ORG: :width 900
[[file:img/small_world_two.png]]

#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://github.com/betanalpha/knitr_case_studies/raw/master/modeling_and_inference/figures/small_world/small_world_two/small_world_two.png" alt="Drawing" width="75%"/></center>
  </div>
#+END_HTML

** The process of inference
   :PROPERTIES:
   :CUSTOM_ID: the-process-of-inference
   :END:

#+ATTR_ORG: :width 900
[[file:img/model_config5.png]]

#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://github.com/betanalpha/knitr_case_studies/raw/master/modeling_and_inference/figures/inferential_config/model_config/model_config5/model_config5.png" alt="Drawing" width="90%"/></center>
  </div>
#+END_HTML

How can we do our best to validate this process works as close as
possible to providing a high quality mirror for natural systems?

* Workflow overview
  :PROPERTIES:
  :CUSTOM_ID: workflow-overview
  :END:

#+ATTR_ORG: :width 900
[[file:img/all.png]]

#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://github.com/betanalpha/knitr_case_studies/raw/master/principled_bayesian_workflow/figures/workflow/all/all.png" alt="Drawing" width="90%"/></center>
  </div>
#+END_HTML

* Load libraries
  :PROPERTIES:
  :CUSTOM_ID: load-libraries
  :END:

#+BEGIN_SRC python
  # %pylab inline
  # import matplotlib.pyplot as plt
  import pymc3 as pm
  import pandas as pd
  import scipy.stats as stats
  import seaborn as sns
  import theano.tensor as T
  import theano
  import numpy as np
  # plt.style.use(['seaborn-talk'])
  # plt.rcParams["figure.figsize"] = (10,8)
  print(pm.__version__)
  print(theano.__version__)
#+END_SRC

#+BEGIN_EXAMPLE
  Populating the interactive namespace from numpy and matplotlib
#+END_EXAMPLE

#+BEGIN_EXAMPLE
  //anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
    from ._conv import register_converters as _register_converters
#+END_EXAMPLE

#+BEGIN_EXAMPLE
  3.5
  1.0.2
#+END_EXAMPLE

** define colors
   :PROPERTIES:
   :CUSTOM_ID: define-colors
   :END:

#+BEGIN_SRC python
  c_light ="#DCBCBC"
  c_light_highlight ="#C79999"
  c_mid ="#B97C7C"
  c_mid_highlight ="#A25050"
  c_dark ="#8F2727"
  c_dark_highlight ="#7C0000"
#+END_SRC

* Section 4.1
  :PROPERTIES:
  :CUSTOM_ID: section-41
  :END:

Build a model that generates (Poisson) counts that may explain what we
have in our data

** Example generative models
   :PROPERTIES:
   :CUSTOM_ID: example-generative-models
   :END:

*** Univariate normal model
    :PROPERTIES:
    :CUSTOM_ID: univariate-normal-model
    :END:

From a very simple perspective, generative modeling refers to the
situation in which we develop a candidate probabilistic specification of
the process from which our data are generated. Usually this will include
the specification of prior distributions over all first-order
parameters.

#+ATTR_ORG: :width 200
[[file:img/tikz-57bc0c88a2974f4c1e2335fe9edb88ff2efdf970.png]]

#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://www.bayespy.org/_images/tikz-57bc0c88a2974f4c1e2335fe9edb88ff2efdf970.png" style="background-color:white;" alt="Drawing" width="10%"/></center>
  </div>
#+END_HTML

\begin{split} p(\mathbf{y}|\mu,\tau) &= \prod^{9}_{n=0}
\mathcal{N}(y_n|\mu,\tau) \ p(\mu) &= \mathcal{N}(\mu|0,10^{-6}) \
p(\tau) &= \mathcal{G}(\tau|10^{-6},10^{-6}) \end{split}

This happens to come from a library called
[[https://github.com/bayespy/bayespy/blob/develop/doc/source/user_guide/quickstart.rst][bayespy]].
The best description I am aware of the syntax and semantics of graphical
models via factor graph notation is in the
[[https://github.com/jluttine/tikz-bayesnet][tikz-bayesnet]] library
[[https://github.com/jluttine/tikz-bayesnet/blob/master/dietz-techreport.pdf][technical
report]].

*** Multivariate normal models
    :PROPERTIES:
    :CUSTOM_ID: multivariate-normal-models
    :END:

#+ATTR_ORG: :width 200
[[file:img/tikz-80a1db369be1f25b61ceacfff551dae2bdd331c3.png]]

#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://www.bayespy.org/_images/tikz-80a1db369be1f25b61ceacfff551dae2bdd331c3.png" style="background-color:white;" alt="Drawing" width="10%"/></center>
  </div>
#+END_HTML

$$\mathbf{y}_{mn} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Lambda}),\qquad m=0,\ldots,9, \quad n=0,\ldots,29.$$

#+ATTR_ORG: :width 300
[[file:img/tikz-97236981a2be663d10ade1ad85caa727621615db.png]]

#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://www.bayespy.org/_images/tikz-97236981a2be663d10ade1ad85caa727621615db.png" style="background-color:white;" alt="Drawing" width="20%"/></center>
  </div>
#+END_HTML

$$\mathbf{y}_{mn} \sim \mathcal{N}(\boldsymbol{\mu}_m,
\mathbf{\Lambda}_n),\qquad m=0,\ldots,9, \quad n=0,\ldots,29.$$

Note that these are for illustrative purposes of the manner in which our
data can share parameters and we have not yet defined priors over our
parameters.

** Our data
   :PROPERTIES:
   :CUSTOM_ID: our-data
   :END:

#+BEGIN_SRC python
  df = pd.read_csv('data.csv')
  print(df.head())
  df.shape
#+END_SRC

** Build a generative model
   :PROPERTIES:
   :CUSTOM_ID: build-a-generative-model
   :END:

#+BEGIN_SRC python
  lbda  = np.linspace(0, 20, num=int(20/0.001))

  plt.plot(lbda, stats.norm(loc=0,scale=6.44787).pdf(lbda), c=c_dark_highlight, lw=2)
  plt.xlabel("lambda"); plt.ylabel("Prior Density"); plt.yticks([]);


  lbda99 = np.linspace(0, 15, num=int(15/0.001))



  plt.fill_between(lbda99,0.,y2=stats.norm(loc=0,scale=6.44787).pdf(lbda99),color=c_dark);
  plt.savefig("fig/prior-density-lambda.svg", bbox_inches="tight");
  !inkscape fig/prior-density-lambda.svg --export-filename=fig/prior-density-lambda.pdf;
#+END_SRC

#+BEGIN_EXAMPLE
  <matplotlib.collections.PolyCollection at 0x1c18baab38>
#+END_EXAMPLE

[[file:55f01ed0ac3c68fb4c27c3d548b423d08fab382e.png]]

In this case the complete Bayesian model is given by

$$
\pi( y_{1}, \ldots, y_{N}, \lambda )
=
\left[ \prod_{n = 1}^{N} \text{Poisson} (y_{n} \mid \lambda) \right]
\cdot \text{HalfNormal} (\lambda \mid 6).
$$

#+ATTR_ORG: :width 900
[[file:img/dgm.png]]

#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://github.com/betanalpha/knitr_case_studies/raw/master/principled_bayesian_workflow/figures/iter1/dgm/dgm.png" alt="Drawing" width="40%"/></center>
  </div>
#+END_HTML

#+BEGIN_SRC python
  #WORKING

  model = pm.Model()
  N = 1000
  R = 500
  with model:
      lbda = pm.HalfNormal("lbda",sd=6.44787)
      
      y = pm.Poisson("y",mu=lbda,shape=(N,),observed=None)
      
#+END_SRC

#+BEGIN_SRC python
  with model:
      trace = pm.sample_prior_predictive(samples=R)
#+END_SRC

#+BEGIN_SRC python
  simu_lbdas = trace['lbda']
  simu_ys = trace['y']
#+END_SRC

#+BEGIN_SRC python
  print(simu_lbdas[0:9])
  print(simu_lbdas.shape)
#+END_SRC

#+BEGIN_SRC python
  print(simu_ys[0:9])
  print(simu_ys.shape)
#+END_SRC

** Plot prior predictive distribution
   :PROPERTIES:
   :CUSTOM_ID: plot-prior-predictive-distribution
   :END:

#+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 1, simu_ys)

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=0)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)
#+END_SRC

#+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Prior predictive distribution');
#+END_SRC

[[file:2d2f1c1c1d2ac3ff23bbe243f87fb1b6d358498d.png]]

** Fit to simulated data
   :PROPERTIES:
   :CUSTOM_ID: fit-to-simulated-data
   :END:

Betancourt performs this for each =y= in trace. For now we just do it
for a single one.

#+BEGIN_SRC python
  model = pm.Model()
  with model:
      lbda = pm.HalfNormal("lbda",sd=6.44787)
      
      y = pm.Poisson("y",mu=lbda,shape=(N,),observed=simu_ys[-1,:])
      
      trace = pm.sample(draws=R,tune=4*R)
     
#+END_SRC

#+BEGIN_EXAMPLE
  Auto-assigning NUTS sampler...
  Initializing NUTS using jitter+adapt_diag...
  Multiprocess sampling (2 chains in 2 jobs)
  NUTS: [lbda]
  Sampling 2 chains: 100%|██████████| 5000/5000 [00:02<00:00, 2450.38draws/s]
#+END_EXAMPLE

#+BEGIN_SRC python
  pm.plots.traceplot(trace);
#+END_SRC

[[file:166dfbee702ffc051d466f75e4636037c4f1b892.png]]

#+ATTR_ORG: :width 900
[[file:img/prior_post_regimes.png]]

#+ATTR_ORG: :width 900
[[file:img/eye_chart_regimes.png]]

#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://github.com/betanalpha/knitr_case_studies/raw/master/principled_bayesian_workflow/figures/eye_chart/prior_post_regimes/prior_post_regimes.png" alt="Drawing" width="70%"/></center>
  </div>
#+END_HTML

#+BEGIN_HTML
  <div>
  <center>    
  <img src="https://github.com/betanalpha/knitr_case_studies/raw/master/principled_bayesian_workflow/figures/eye_chart/eye_chart_regimes.png" alt="Drawing" width="70%"/></center>
  </div>
#+END_HTML

Posterior z-score
$$z[f \mid \tilde{y}, \theta^{\dagger}] =
\frac{ \mathbb{E}_{\mathrm{post}}[f \mid \tilde{y}] - f(\theta^{\dagger}) }
{ \mathbb{E}_{\mathrm{post}}[f \mid \tilde{y} ] },$$

Posterior contraction
$$
c[f \mid \tilde{y}] = 1 -
\frac{ \mathbb{V}_{\mathrm{post}}[f \mid \tilde{y} ] }
{ \mathbb{V}_{\mathrm{prior}}[f \mid \tilde{y} ] },
$$

#+BEGIN_SRC python
  # Compute rank of prior draw with respect to thinned posterior draws
  sbc_rank = np.sum(simu_lbdas < trace['lbda'][::2])
#+END_SRC

#+BEGIN_EXAMPLE
  (500,)
#+END_EXAMPLE

#+BEGIN_SRC python
  # posterior sensitivities analysis
  s = pm.stats.summary(trace,varnames=['lbda'])
  post_mean_lbda = s['mean'].values
  post_sd_lbda = s['sd'].values
  prior_sd_lbda = 6.44787
  z_score = np.abs((post_mean_lbda - simu_lbdas) / post_sd_lbda)
  shrinkage = 1 - (post_sd_lbda / prior_sd_lbda ) ** 2
#+END_SRC

#+BEGIN_SRC python
  plt.plot(shrinkage[0]*np.ones(len(z_score)),z_score,'o',c="#8F272720");
  plt.xlim(0,1.01); plt.xlabel('Posterior shrinkage'); plt.ylabel('Posterior z-score');
#+END_SRC

[[file:210fea12128d3fd74d92569db070a682d01a20d0.png]]

** Fit observations and evaluate
   :PROPERTIES:
   :CUSTOM_ID: fit-observations-and-evaluate
   :END:

#+BEGIN_SRC python
  df = pd.read_csv('data.csv')
  data_ys = df[df['data']=='y']['value'].values
#+END_SRC

#+BEGIN_SRC python
  model = pm.Model()
  with model:
      lbda = pm.HalfNormal("lbda",sd=6.44787)
      
      y = pm.Poisson("y",mu=lbda,shape=(N,),observed=data_ys)
      
      trace = pm.sample(draws=R,tune=4*R,chains=4)
#+END_SRC

#+BEGIN_EXAMPLE
  Auto-assigning NUTS sampler...
  Initializing NUTS using jitter+adapt_diag...
  Multiprocess sampling (4 chains in 2 jobs)
  NUTS: [lbda]
  Sampling 4 chains: 100%|██████████| 10000/10000 [00:05<00:00, 1684.30draws/s]
#+END_EXAMPLE

#+BEGIN_SRC python
  pm.plots.plot_posterior(trace,varnames=['lbda']);
#+END_SRC

[[file:399203844de13168a6e2d4e52da87219e6f133b9.png]]

#+BEGIN_SRC python
  with model:
       ppc = pm.sample_posterior_predictive(trace)
#+END_SRC

#+BEGIN_EXAMPLE
  100%|██████████| 500/500 [00:00<00:00, 1507.74it/s]
#+END_EXAMPLE

#+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 1, ppc['y'])

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=0)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)

  data_hist = np.histogram(data_ys,bins=bins)[0]
  data_hist_interp = np.repeat(data_hist, 10)
#+END_SRC

#+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.plot(bin_interp,data_hist_interp,color='black');
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Posterior predictive distribution');
#+END_SRC

[[file:9968c94812b58c58b09bdbbcf8ec4fc10e42957a.png]]

* Section 4.2
  :PROPERTIES:
  :CUSTOM_ID: section-42
  :END:

#+BEGIN_SRC python
  generative_ensemble2 = pm.Model()

  N = 1000
  R = 1000

  with generative_ensemble2:
      theta = pm.Beta(name="theta", alpha = 1, beta = 1)
      lambda_ = pm.HalfNormal(name="lambda", sd = 6.44787)
      y = pm.ZeroInflatedPoisson(name = "y", psi = theta, theta = lambda_, shape = (N,))
#+END_SRC

#+BEGIN_SRC python
  with generative_ensemble2:
      trace = pm.sample_prior_predictive(samples=R)
#+END_SRC

#+BEGIN_SRC python
  trace["theta"][:10]
#+END_SRC

#+BEGIN_EXAMPLE
  array([0.93332145, 0.7714699 , 0.86558357, 0.86497487, 0.60348962,
         0.74533668, 0.72284839, 0.03031713, 0.79020511, 0.34467172])
#+END_EXAMPLE

#+BEGIN_SRC python
  trace["lambda"][:10]
#+END_SRC

#+BEGIN_EXAMPLE
  array([11.79980369,  0.07890616,  7.9198449 ,  4.02674117,  0.64993403,
          1.55746614,  3.21968629,  2.54351671,  0.94194133,  1.09817687])
#+END_EXAMPLE

#+BEGIN_SRC python
  simu_ys = trace["y"]
  simu_ys
#+END_SRC

#+BEGIN_EXAMPLE
  array([[ 0,  0,  0, ...,  6,  0,  0],
         [ 0,  0,  0, ...,  5,  1, 11],
         [ 0,  0,  0, ...,  7,  3,  6],
         ...,
         [ 0,  0,  0, ...,  5,  3, 16],
         [ 0,  0,  0, ...,  5,  0,  6],
         [ 0,  0,  0, ...,  7,  1, 11]])
#+END_EXAMPLE

#+BEGIN_SRC python
  np.count_nonzero(simu_ys, axis=0).std()
#+END_SRC

#+BEGIN_EXAMPLE
  285.0109788692358
#+END_EXAMPLE

#+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0 ,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)

  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 1, simu_ys.T)

  prctiles = np.percentile(hists,np.linspace(10, 90,num=9),axis=0)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)



  for i, color in enumerate([c_light, c_light_highlight, c_mid, c_mid_highlight]):
      plt.fill_between(bin_interp, prctiles_interp[i, :],
                       prctiles_interp[-1 - i, :],
                       alpha = 1.0,
                       color = color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Prior predictive distribution');
#+END_SRC

[[file:2a8494a54cb0c2a826ce359e7d7be9660ab6cb0d.png]]

#+BEGIN_SRC python
  simu_ys[simu_ys > 25].size / simu_ys.size
#+END_SRC

#+BEGIN_EXAMPLE
  0.000609
#+END_EXAMPLE

** Fit Simulated Observations and Evaluate
   :PROPERTIES:
   :CUSTOM_ID: fit-simulated-observations-and-evaluate
   :END:

#+BEGIN_SRC python
  fit_data2 = pm.Model()

  N = 1000
  R = 1000

  with fit_data2:
      theta = pm.Beta(name="theta", alpha = 1, beta = 1)
      lambda_ = pm.HalfNormal(name="lambda", sd = 6.44787)
      y = pm.ZeroInflatedPoisson(name = "y", 
                                 psi = theta, 
                                 theta = lambda_, 
                                 shape = (N,),
                                 observed=simu_ys[-1,:])
#+END_SRC

#+BEGIN_SRC python
  with fit_data2:
      trace_fit = pm.sample(R)
#+END_SRC

#+BEGIN_EXAMPLE
  Auto-assigning NUTS sampler...
  Initializing NUTS using jitter+adapt_diag...
  Multiprocess sampling (4 chains in 4 jobs)
  NUTS: [lambda, theta]
  Sampling 4 chains: 100%|██████████| 6000/6000 [00:01<00:00, 3320.34draws/s]
#+END_EXAMPLE

#+BEGIN_SRC python
  pm.plots.traceplot(trace_fit)
#+END_SRC

#+BEGIN_EXAMPLE
  array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fcf6a29acc0>,
          <matplotlib.axes._subplots.AxesSubplot object at 0x7fcf6a2a0be0>],
         [<matplotlib.axes._subplots.AxesSubplot object at 0x7fcf6a22b4a8>,
          <matplotlib.axes._subplots.AxesSubplot object at 0x7fcf6a2376a0>]],
        dtype=object)
#+END_EXAMPLE

[[file:7c6d31b3f1a6e79242549a0555cad6c80e852686.png]]

#+BEGIN_SRC python
  pm.summary(trace_fit, varnames=["theta", "lambda"]).round(2)
#+END_SRC

#+BEGIN_EXAMPLE
          mean    sd  mc_error  hpd_2.5  hpd_97.5    n_eff  Rhat
  theta   0.43  0.02       0.0     0.41      0.46  4174.79   1.0
  lambda  6.06  0.12       0.0     5.83      6.29  4191.15   1.0
#+END_EXAMPLE

#+BEGIN_SRC python
#+END_SRC

#+BEGIN_SRC python
  import pickle
  with open("fit_data2.pkl", "wb+") as buffer:
      pickle.dump({"model": fit_data2, "trace": trace_fit}, buffer)
#+END_SRC

* Section 4.3
  :PROPERTIES:
  :CUSTOM_ID: section-43
  :END:

Build a model that generates zero-inflated Poisson counts

** Build a generative model
   :PROPERTIES:
   :CUSTOM_ID: build-a-generative-model
   :END:

#+BEGIN_SRC python
  lbda  = np.linspace(0, 20, num=int(20/0.001))
  pdf = stats.invgamma(3.48681,scale=9.21604)
  plt.plot(lbda, pdf.pdf(lbda), c=c_dark_highlight, lw=2)
  plt.xlabel("lambda"); plt.ylabel("Prior Density"); plt.yticks([]);


  lbda99 = np.linspace(1, 15, num=int(15/0.001))



  plt.fill_between(lbda99,0.,y2=pdf.pdf(lbda99),color=c_dark)
#+END_SRC

#+BEGIN_EXAMPLE
  <matplotlib.collections.PolyCollection at 0x1c16f5ab38>
#+END_EXAMPLE

[[file:ec9c30b79f92157763282b760026d82d42ae54d7.png]]

#+BEGIN_SRC python
  theta  = np.linspace(0, 1, num=int(1/0.001))
  pdf = stats.beta(2.8663,2.8663)
  plt.plot(theta, pdf.pdf(theta), c=c_dark_highlight, lw=2)
  plt.xlabel("theta"); plt.ylabel("Prior Density"); plt.yticks([]);


  theta99 = np.linspace(0.1, 0.9, num=int(0.8/0.001))



  plt.fill_between(theta99,0.,y2=pdf.pdf(theta99),color=c_dark)
#+END_SRC

#+BEGIN_EXAMPLE
  <matplotlib.collections.PolyCollection at 0x1c174559b0>
#+END_EXAMPLE

[[file:7702024a502053ca16359b00681e39b43b4f277d.png]]

#+BEGIN_SRC python
  #WORKING

  model = pm.Model()
  N = 1000
  R = 1000
  with model:
      lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
      theta = pm.Beta("theta",alpha=2.8663,beta=2.8663)
      
      y = pm.ZeroInflatedPoisson("y",psi=theta,theta=lbda,shape=N)
      
#+END_SRC

#+BEGIN_SRC python
  # Note this breaks when N != R
  with model:
      trace = pm.sample_prior_predictive(samples=R)
#+END_SRC

#+BEGIN_SRC python
  simu_lbdas = trace['lbda']
  simu_thetas = trace['theta']
  simu_ys = trace['y']
#+END_SRC

** Plot prior predictive distribution
   :PROPERTIES:
   :CUSTOM_ID: plot-prior-predictive-distribution
   :END:

#+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 0, simu_ys)

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=1)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)
#+END_SRC

#+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Prior predictive distribution');
#+END_SRC

[[file:84a7a4175bc79a893e1e6eec9f327d292b86a03c.png]]

** Fit to simulated data
   :PROPERTIES:
   :CUSTOM_ID: fit-to-simulated-data
   :END:

In example Betancourt performs this for each =y= in trace. Here we just
do it for one.

#+BEGIN_SRC python
  model = pm.Model()
  with model:
      lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
      theta = pm.Beta("theta",alpha=2.8663,beta=2.8663)
      
      y = pm.ZeroInflatedPoisson("y",psi=theta,theta=lbda,shape=N,observed=simu_ys[:,-1])
      
      trace = pm.sample(draws=R,tune=4*R)
     
#+END_SRC

#+BEGIN_EXAMPLE
  Auto-assigning NUTS sampler...
  Initializing NUTS using jitter+adapt_diag...
  Multiprocess sampling (2 chains in 2 jobs)
  NUTS: [theta, lbda]
  Sampling 2 chains: 100%|██████████| 10000/10000 [00:06<00:00, 1599.00draws/s]
#+END_EXAMPLE

#+BEGIN_SRC python
  pm.plots.traceplot(trace);
#+END_SRC

[[file:2520f42e01c94a288e665d2a67afe49ddfb319b3.png]]

#+BEGIN_SRC python
  # Compute rank of prior draw with respect to thinned posterior draws
  sbc_rank = np.sum(simu_lbdas < trace['lbda'][::2])
#+END_SRC

#+BEGIN_SRC python
  # posterior sensitivities analysis
  s = pm.stats.summary(trace,varnames=['lbda'])
  post_mean_lbda = s['mean'].values
  post_sd_lbda = s['sd'].values
  prior_sd_lbda = 6.44787
  z_score = np.abs((post_mean_lbda - simu_lbdas) / post_sd_lbda)
  shrinkage = 1 - (post_sd_lbda / prior_sd_lbda ) ** 2
#+END_SRC

#+BEGIN_SRC python
  plt.plot(shrinkage[0]*np.ones(len(z_score)),z_score,'o',c="#8F272720");
  plt.xlim(0,1.01); plt.xlabel('Posterior shrinkage'); plt.ylabel('Posterior z-score');
#+END_SRC

[[file:72da9d55dbc00ea3d386105e8da8cf92ab2cb645.png]]

** Fit observations and evaluate
   :PROPERTIES:
   :CUSTOM_ID: fit-observations-and-evaluate
   :END:

#+BEGIN_SRC python
  df = pd.read_csv('data.csv')
  data_ys = df[df['data']=='y']['value'].values
#+END_SRC

#+BEGIN_SRC python
  model = pm.Model()
  with model:
      lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
      theta = pm.Beta("theta",alpha=2.8663,beta=2.8663)
      
      y = pm.ZeroInflatedPoisson("y",psi=theta,theta=lbda,shape=N,observed=data_ys)
      
      trace = pm.sample(draws=R,tune=4*R,chains=4)
#+END_SRC

#+BEGIN_EXAMPLE
  Auto-assigning NUTS sampler...
  Initializing NUTS using jitter+adapt_diag...
  Multiprocess sampling (4 chains in 2 jobs)
  NUTS: [theta, lbda]
  Sampling 4 chains: 100%|██████████| 20000/20000 [00:12<00:00, 1581.64draws/s]
#+END_EXAMPLE

#+BEGIN_SRC python
  pm.plots.plot_posterior(trace,varnames=['lbda']);
#+END_SRC

[[file:170b6bd75b092475c2e37d03567c46c68faa3966.png]]

#+BEGIN_SRC python
  with model:
       ppc = pm.sample_ppc(trace)
#+END_SRC

#+BEGIN_EXAMPLE
  100%|██████████| 1000/1000 [00:00<00:00, 1262.27it/s]
#+END_EXAMPLE

#+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 0, ppc['y'])

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=1)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)

  data_hist = np.histogram(data_ys,bins=bins)[0]
  data_hist_interp = np.repeat(data_hist, 10)
#+END_SRC

#+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.plot(bin_interp,data_hist_interp,color='black');
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Posterior predictive distribution');
#+END_SRC

[[file:13ad70109db68c9e9bda363af30d0fb01b9b6259.png]]

* Section 4.4
  :PROPERTIES:
  :CUSTOM_ID: section-44
  :END:

#+BEGIN_SRC python
  from pymc3.distributions.distribution import generate_samples,draw_values,Discrete
  from pymc3.distributions.discrete import Poisson

  def rv_truncated_poisson(mu,mx, size=None):
      mu = np.asarray(mu)
      mx = np.asarray(mx)
      dist = stats.distributions.poisson(mu)

      lower_cdf = 0.
      upper_cdf = dist.cdf(mx)
      nrm = upper_cdf - lower_cdf
      sample = np.random.random_sample(size) * nrm + lower_cdf

      return dist.ppf(sample)

  class TruncatedZeroInflatedPoisson(Discrete):

      def __init__(self, mu, mx, psi, *args, **kwargs):
          super(TruncatedZeroInflatedPoisson, self).__init__(*args, **kwargs)
          self.mu  = tt.as_tensor_variable(mu)
          self.mx = tt.as_tensor_variable(mx)
          self.psi = tt.as_tensor_variable(psi)
          self.mode = tt.floor(mu).astype('int32')


      def random(self, point=None, size=None):
          mu, psi, mx = draw_values([self.mu, self.psi, self.mx], point=point, size=size)
          g = generate_samples(rv_truncated_poisson, mu,mx,
                               dist_shape=self.shape,
                               size=size)
          return g * (np.random.random(np.squeeze(g.shape)) < psi)

      def logp(self, value):
          psi = self.psi
          mu = self.mu
          mx = self.mx
          poisson = pm.Poisson.dist(mu)
          logp_val = tt.switch(
              tt.gt(value, 0),
              tt.log(psi) + poisson.logp(value),
              pm.math.logaddexp(tt.log1p(-psi), tt.log(psi) - mu))

          return pm.distributions.dist_math.bound(
              logp_val,
              0 <= value,
              value <= mx,
              0 <= psi, psi <= 1,
              0 <= mu)
#+END_SRC

#+BEGIN_SRC python
  model = pm.Model()
  N = 1000
  R = 1000
  with model:
      lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
      psi = pm.Beta("psi",alpha=2.8663,beta=2.8663)
      
      y = TruncatedZeroInflatedPoisson("y",psi=psi,mu=lbda,mx=15.,shape=N)
#+END_SRC

#+BEGIN_SRC python
  with model:
      trace = pm.sample_prior_predictive(samples=1000)
#+END_SRC

#+BEGIN_SRC python
  simu_lbdas = trace['lbda']
  simu_thetas = trace['psi']
  simu_ys = trace['y']
#+END_SRC

#+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 0, simu_ys)

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=1)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)
#+END_SRC

#+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Prior predictive distribution');
#+END_SRC

[[file:9fc4b1629fd08cbff6013230d6356cd1716f3763.png]]

#+BEGIN_SRC python
  model = pm.Model()
  N = 1000
  R = 1000
  with model:
      lbda = pm.InverseGamma("lbda",alpha=3.48681,beta=9.21604)
      psi = pm.Beta("psi",alpha=2.8663,beta=2.8663)
      
      y = TruncatedZeroInflatedPoisson("y",psi=psi,mu=lbda,mx=14.,shape=N,observed=data_ys)
      trace = pm.sample(draws=R,tune=4*R,chains=4)    
#+END_SRC

#+BEGIN_EXAMPLE
  Auto-assigning NUTS sampler...
  Initializing NUTS using jitter+adapt_diag...
  Multiprocess sampling (4 chains in 2 jobs)
  NUTS: [psi, lbda]
  Sampling 4 chains: 100%|██████████| 20000/20000 [00:13<00:00, 1485.98draws/s]
#+END_EXAMPLE

#+BEGIN_SRC python
  pm.plots.plot_posterior(trace);
#+END_SRC

[[file:f94277f8a0757861a28c3c460e9effc71fba7ad3.png]]

#+BEGIN_SRC python
  with model:
       ppc = pm.sample_ppc(trace)
#+END_SRC

#+BEGIN_EXAMPLE
  100%|██████████| 1000/1000 [00:06<00:00, 143.41it/s]
#+END_EXAMPLE

#+BEGIN_SRC python
  x_max = 30
  bins = np.arange(0,x_max)
  bin_interp = np.linspace(0,x_max-1,num=(x_max-1)*10)
  hists = np.apply_along_axis(lambda a: np.histogram(a, bins=bins)[0], 0, ppc['y'])

  prctiles = np.percentile(hists,np.linspace(10,90,num=9),axis=1)
  prctiles_interp = np.repeat(prctiles, 10,axis=1)

  data_hist = np.histogram(data_ys,bins=bins)[0]
  data_hist_interp = np.repeat(data_hist, 10)
#+END_SRC

#+BEGIN_SRC python
  for i,color in enumerate([c_light,c_light_highlight,c_mid,c_mid_highlight]):
      plt.fill_between(bin_interp,prctiles_interp[i,:],prctiles_interp[-1-i,:],alpha=1.0,color=color);


  plt.plot(bin_interp,prctiles_interp[4,:],color=c_dark_highlight);
  plt.plot(bin_interp,data_hist_interp,color='black');
  plt.axvline(x=25,ls='-',lw=2,color='k');
  plt.xlabel('y');
  plt.title('Posterior predictive distribution');
#+END_SRC

[[file:ae5e774a1ebb6c159c5a5203430ecab17bc9f5fa.png]]
